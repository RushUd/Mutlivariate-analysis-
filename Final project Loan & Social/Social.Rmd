---
title: "Final-Social"
output:
  html_document:
    df_print: paged
---

```{r}
library(readr)
library(FactoMineR)
library(ggfortify)
library(psych)
library(corrplot)
library(devtools)
data <- read.csv("C:/Users/rusha/Downloads/Multivariate/mva class social media.csv")
str(data)
```

```{r}
#data$OTT <- data$OTT..Netflix..Hulu..Prime.video..
summary(data)
```


```{r}
stddata <- scale(data)
stddata
```


```{r}
datacov <- cov(data)
datacov

# Formatting 
datacov_rounded <- round(datacov, digits = 2)
datacov_df <- as.data.frame(datacov_rounded)
datacov_df
```

```{r}
datamean <-  colMeans(data)
datamean
```

```{r}
data_MD <- mahalanobis(data, datamean, datacov)
data_MD

#My usage vs class
data_MD[4]

```

```{r}
# Lets plot correlation between columns
cor.plot(data)
```

Insights: 
1) Instagram is highly correlated with Snapchat, Whatsapp/Wechat.
2) Tired waking up is correlated with Trouble falling asleep.
3) Trouble falling asleep is correlated with Instagram, Snapchat.
4) Tired waking up is correlated with Linkedin, Youtube but in opposite direction. 


```{r}
#Lets identify outliers in our data
boxplot(data)
```

## Principal Component Analysis

```{r}
cor(data)
```

```{r}
data_pca <- prcomp(data,scale. =TRUE)
data_pca
```

```{r}
summary(data_pca)
```

Insights : Keeping PC1, PC2, PC3 and PC4, as it captures ~75% of variance. 

## Checking if our insight is correct 

```{r}
(eigen_data <- data_pca$sdev^2)
```

```{r}
plot(eigen_data, xlab = "Component number", ylab = "Component variance", type = "l", main = "Scree diagram")

```
```{r}
plot(eigen_data, type = "b", main="Scree Plot")
abline(h=1,col="blue",lty=2)
```

```{r}
library(factoextra)
fviz_eig(data_pca, addlabels = TRUE)
```

```{r}

fviz_pca_var(data_pca,col.var = "cos2",
             gradient.cols = c("#FFCC00", "#CC9933", "#660033", "#330033"),
             repel = TRUE)
```

Plot shows which variables are correlated. Here, Linkedin, Snapchat, Instagram, Youtube, Whatsapp/Wechat, Mood Productivity.

## Clustering 
#1) Hierarchical

```{r}
library(cluster)
library(readr)
library(factoextra)
library(magrittr)
library(NbClust)

dist.data <- dist(stddata, method="euclidean")
colnames(dist.data) <- rownames(dist.data)

clusdata.nn <- hclust(dist.data, method = "single")

options(repr.plot.width=10, repr.plot.height=6)  # Adjust the plot size as needed
plot(as.dendrogram(clusdata.nn), ylab="Distance",
     main="Dendrogram of all Users")
```
Insight: I think the optimal number of clusters are 5 

## Non-hierarchical clustering/ K-means clustering

```{r}
(kmeans2.data <- kmeans(stddata,4,nstart = 10))

```

```{r}
gap_stat <- clusGap(stddata, FUN = kmeans, nstart = 10, K.max = 10, B = 50)

fviz_gap_stat(gap_stat)
```

Optimal number of cluster verified with the above plot. 

```{r}
set.seed(123)
## Perform K-means clustering
km.res3 <- kmeans(stddata, 3, nstart = 25)  
# Visualize clusters
fviz_cluster(km.res3, data = stddata,  
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())
```
From the plot we cannot directly infer anything. We see a lot of overlapping clusters. That is because we have a lot users that are using the same social media platforms. 

```{r}
set.seed(123)
## Perform K-means clustering
km.res5 <- kmeans(stddata, 5, nstart = 25)  
# Visualize clusters
fviz_cluster(km.res5, data = stddata,  
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())
```

```{r}
res.hc <- stddata %>% scale() %>% dist(method = "euclidean") %>%
  hclust(method = "ward.D2")  # Change matstd.data to your dataset

# Visualize the Dendrogram
fviz_dend(res.hc, k = 5,  # Cut in four groups
          cex = 0.5,  # label size
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07", "pink"),
          color_labels_by_k = TRUE,  # color labels by groups
          rect = TRUE)
```



## Factor Analysis
```{r}
fa.parallel(data[-1])
```

```{r}
fit.pc <- principal(data[-1], nfactors=3, rotate="varimax")
fit.pc
```

```{r}
fa.diagram(fit.pc)
```

```{r}
vss(data[-1])
```


# Regression

```{r}
social_data <- data
```

```{r}
model_fit <- lm(Mood.Productivity ~ Instagram + Whatsapp..Wechat + Youtube + Linkedin, data = social_data)
summary(model_fit)
coefficients(model_fit)
```
Intercept: 
FOr one unit increase in (Instagram/Whatsapp..Wechat/Linkedin), the Mood/productivity of a person increases by (0.012/0.013/0.032) respectively. And for one unit increase in YouTube, the Mood decreases by 0.04.

From the Adjusted R-squared value, the model only explains an 8.9% of variance in data. 
This is not the best model. 

2. Model Acceptance
```{r}
# Assess the acceptance of the model
# Check R-squared and adjusted R-squared
r_squared <- summary(model_fit)$r.squared
adj_r_squared <- summary(model_fit)$adj.r.squared
cat("R-squared:", r_squared, "\n")
cat("Adjusted R-squared:", adj_r_squared, "\n")

# Check the F-statistic and its p-value
f_statistic <- summary(model_fit)$fstatistic
f_statistic_value <- f_statistic[1]
f_statistic_p_value <- pf(f_statistic_value, f_statistic[2], f_statistic[3], lower.tail = FALSE)
cat("F-statistic:", f_statistic_value, "\n")
cat("p-value:", f_statistic_p_value, "\n")
```

Although, R-squared: 0.271 suggests the model explains moderate amount of variance, the F-statistic is 1.49 and p-value is 0.2511, indicating that the model is not significant.

3. Residual Analysis 
```{r}
# Select only numeric variables from fish_data
numeric_social_data <- social_data[, sapply(social_data, is.numeric)]
# Plot pairs
pairs(numeric_social_data, main = "Loan Data")
confint(model_fit,level=0.95)
fitted(model_fit)
residuals(model_fit)

# Plot residuals vs. fitted values
plot(model_fit, which = 1)

# Plot normal Q-Q plot of residuals
plot(model_fit, which = 2)
```

Most of the points lies on the straight line, thus the points are normally distributed.

4. Prediction
```{r}
# Make predictions
predictions <- predict(model_fit, newdata = social_data)

# Print predictions
print(predictions)
```

5. Model Accuracy
```{r}
# Calculate Mean Squared Error (MSE)
mse <- mean(model_fit$residuals^2)

# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mse)

# Calculate Mean Absolute Error (MAE)
mae <- mean(abs(model_fit$residuals))

# Print the accuracy metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
```

Lower values of MSE, RMSE, and MAE indicate better model performance, with smaller differences between actual and predicted values.


```{r}
#Anova Table
anova(model_fit)
vcov(model_fit)
cov2cor(vcov(model_fit))
temp <- influence.measures(model_fit)
temp
plot(model_fit)

anova_result <- anova(model_fit)
print(anova_result)
```


# Logistic Regression 

```{r}
library(ggplot2)
library(cowplot)
library(caret)
library(e1071)
library(pROC)
```

```{r}
sapply(social_data, function(x) any(is.na(x)))
social_data$Mood.Productivity <- as.factor(social_data$Mood.Productivity)
social_data$Tired.waking.up.in.morning <- as.factor(social_data$Tired.waking.up.in.morning)
social_data$Trouble.falling.asleep <- as.factor(social_data$Trouble.falling.asleep)
str(social_data)
```

#1. Model Development
```{r}
logistic_simple1 <- glm(Tired.waking.up.in.morning ~ Instagram, data=social_data, family="binomial")
summary(logistic_simple1)
```
```{r}
logistic_simple2 <- glm(Tired.waking.up.in.morning ~ Linkedin, data=social_data, family="binomial")
summary(logistic_simple2)
```

Selecting logistic regression model 2 as my model, has lower AIC value. 


```{r}
logistic <- glm(Tired.waking.up.in.morning ~ ., data=social_data, family="binomial")
summary(logistic)
```

2) Model Acceptance 
3) Residual Analysis and Prediction
```{r}
#let's  see what this logistic regression predicts
predicted.data <- data.frame(probability.of.tiredwaking=logistic_simple2$fitted.values,Linkedin=social_data$Linkedin)
predicted.data

#predicted.data <- data.frame(probability.of.ls=logistic$fitted.values,ls=loan_data$loan_status)
predicted.data <- predicted.data[order(predicted.data$probability.of.tiredwaking, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
## Lastly, we can plot the predicted probabilities for each sample having
## heart disease and color by whether or not they actually had heart disease
ggplot(data=predicted.data, aes(x=rank, y=probability.of.tiredwaking)) + geom_point(aes(color=Linkedin), alpha=1, shape=4, stroke=2) + xlab("Index") + ylab("Predicted probability of waking up tired")
```


5) Model Accuracy 
```{r}
library(caret)
# Confusion matrix
conf_mat <- table(Actual = ifelse(social_data$Tired.waking.up.in.morning == "1", "Tired", "Not-tired"), Predicted = ifelse(logistic$fitted.values > 0.5, "Tired", "Not-tired"))
print(conf_mat)

# Calculate accuracy
accuracy <- sum(diag(conf_mat)) / sum(conf_mat)
print(paste("Accuracy:", accuracy))

# Calculate precision
precision <- posPredValue(conf_mat)
print(paste("Precision:", precision))

# Calculate recall
recall <- conf_mat["Tired", "Tired"] / sum(conf_mat["Tired", ])
print(paste("Recall:", recall))

# ROC curve
roc_obj <- roc(social_data$Linkedin, logistic$fitted.values)
plot(roc_obj, xlab="False Positive Percentage", ylab="True Postive Percentage", legacy.axes=TRUE, col="#377eb8", lwd=4, percent=TRUE, print.auc=TRUE)
```

The accuracy of the model is ~86%, that is model correctly predicts whether a person feels tired or not while waking up in the morning. 

The precision of the model is ~93% 

AUC is 1, this means the model has excellent ability to distinguish between person feeling tired vs not tired. 

---
title: "R Notebook"
output: html_notebook
---
---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

1) Model development
```{r}
library(MASS)
library(ggplot2)
library(memisc)
library(ROCR)
library(dplyr)
library(klaR)

str(social_data)
```

```{r}
#wdbc.data <- as.matrix(wdbc[,c(3:32)])
#row.names(wdbc.data) <- wdbc$id
#wdbc_raw <- cbind(wdbc.data, as.numeric(as.factor(wdbc$diagnosis))-1)
#smp_size_raw <- floor(0.75 * nrow(wdbc_raw))
#train_ind_raw <- sample(nrow(wdbc_raw), size = smp_size_raw)
#train_raw.df <- as.data.frame(wdbc_raw[train_ind_raw, ])
#test_raw.df <- as.data.frame(wdbc_raw[-train_ind_raw, ])
#wdbc_raw.lda <- lda(formula = train_raw.df$diagnosis ~ ., data = train_raw.df)
#wdbc_raw.lda

# Combine loan data with loan status and income group
social.data <- as.matrix(social_data[,c(1:8)])
social.data <- cbind(social.data, tired = social_data$Tired.waking.up.in.morning)
row.names(social.data) <- social_data$Tired.waking.up.in.morning
soc_raw <- cbind(social.data, as.numeric(as.factor(social_data$Tired.waking.up.in.morning))-1)
#colnames(soc_raw)[10] <- "Tired"
# Remove rows with NA values
soc_raw <- soc_raw[complete.cases(soc_raw), ]
# Split data into train and test sets
samp_size_raw <- floor(0.75 * nrow(soc_raw))
train_soc_raw <- sample(nrow(soc_raw), size = samp_size_raw)
train_raw.df <- as.data.frame(soc_raw[train_soc_raw, ])
test_raw.df <- as.data.frame(soc_raw[-train_soc_raw, ])
# Fit LDA model
train_raw.df <- train_raw.df[, -9]
soc_raw.lda <- lda(formula = V10 ~., data = train_raw.df)
soc_raw.lda
```

Insights
The prior probability of group 0 is approximately 0.43, and the prior probability of group 1 is approximately 0.57, indicating that the training dataset is slightly imbalanced, with more observations belonging to group 1.(i.e loan accepted)

loan_int_rate and loan_percent_income have positive coefficients, suggesting that higher values of these variables are associated with group 1.
'V4' or 'income_group' has a negative coefficient, indicating that higher values of V4 are associated with group 0.

```{r}
plot(soc_raw.lda, xlab = "Predicted Group")
```


```{r}
#wdbc_raw.lda.predict <- predict(wdbc_raw.lda, newdata = test_raw.df)
#wdbc_raw.lda.predict$class
#wdbc_raw.lda.predict$x

#loan_raw.lda.predict <- predict(loan_raw.lda, newdata = test_raw.df)
#loan_raw.lda.predict$class
#loan_raw.lda.predict$x

# Remove NA values from the data frame
#clean_test_raw.df <- na.omit(test_raw.df)

# Perform prediction with LDA
soc_raw.lda.predict <- predict(soc_raw.lda, newdata = test_raw.df)

# Access predicted class
soc_raw.lda.predict$class

# Access LD scores
soc_raw.lda.predict$x
```

```{r}
# Get the posteriors as a dataframe.
soc_raw.lda.predict.posteriors <- as.data.frame(soc_raw.lda.predict$posterior)

pred <- prediction(soc_raw.lda.predict.posteriors[,2], test_raw.df$tired)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))

# The AUC value will be different everytime we compute the code as we are taking random sample for test and train model.
```

# Only 22 percent is predicted by the model. 
